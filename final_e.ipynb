{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Semantic Search Pipeline with Hugging Face Datasets\n",
    "\n",
    "Steps:\n",
    "1. Login to Hugging Face Hub\n",
    "2. Load dataset (change DATASET_NAME / DATASET_CONFIG to experiment)\n",
    "3. Clean text and split into sentence-level chunks\n",
    "4. Embed all chunks into 128-dimensional vectors\n",
    "5. Embed a prompt into the same vector space\n",
    "6. Compute Euclidean distance and show closest matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdmahfuzurrahman/vector_db/vectordb/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import nltk  --- for sentence\\ntry:\\n    from nltk.tokenize import sent_tokenize\\n    nltk.data.find(\"tokenizers/punkt\")\\nexcept LookupError:\\n    nltk.download(\"punkt\")\\n    from nltk.tokenize import sent_tokenize'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import nltk  --- for sentence\n",
    "try:\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "    from nltk.tokenize import sent_tokenize\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config: HF token, dataset, model, search\n",
    "\n",
    "HF_TOKEN        = \"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\"\n",
    "\n",
    "DATASET_NAME    = \"sentence-transformers/gooaq\"   # FIXED\n",
    "DATASET_CONFIG  = None                    # no config for this dataset\n",
    "MAX_ROWS        = 1000\n",
    "\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "TOP_K            = 10\n",
    "CHUNK_SIZE       = 60\n",
    "\n",
    "PROMPT           = \"What is the capital of Finland?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in successfully!\n"
     ]
    }
   ],
   "source": [
    "login(HF_TOKEN)\n",
    "print(\"Logged in successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset: sentence-transformers/gooaq\n",
      "Dataset loaded.\n",
      "Number of rows: 1000\n",
      "Columns: ['question', 'answer']\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading dataset: {DATASET_NAME}\"\n",
    "      + (f\"/{DATASET_CONFIG}\" if DATASET_CONFIG is not None else \"\"))\n",
    "\n",
    "if DATASET_CONFIG is not None:\n",
    "    dataset = load_dataset(DATASET_NAME, DATASET_CONFIG, split=\"train\")\n",
    "else:\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "\n",
    "if MAX_ROWS is not None:\n",
    "    dataset = dataset.select(range(min(MAX_ROWS, len(dataset))))\n",
    "\n",
    "print(\"Dataset loaded.\")\n",
    "print(\"Number of rows:\", len(dataset))\n",
    "print(\"Columns:\", dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-empty answers: 1000\n"
     ]
    }
   ],
   "source": [
    "# Extract non-empty text lines\n",
    "raw_texts = []\n",
    "\n",
    "for ans in dataset[\"answer\"]:\n",
    "    if ans:\n",
    "        content = ans.strip()\n",
    "        if content:\n",
    "            raw_texts.append(content)\n",
    "\n",
    "print(\"Non-empty answers:\", len(raw_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Split text into chunks of up to `max_len` characters,\n",
    "    but try not to break words in the middle.\n",
    "    \"\"\"\n",
    "\n",
    "def chunk_text(text: str, max_len: int = CHUNK_SIZE):\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    n = len(text)\n",
    "\n",
    "    while i < n:\n",
    "        end = min(i + max_len, n)\n",
    "        window = text[i:end]\n",
    "\n",
    "        if end < n:\n",
    "            last_space = window.rfind(\" \")\n",
    "            if last_space != -1 and last_space != 0:\n",
    "                end = i + last_space\n",
    "                window = text[i:end]\n",
    "\n",
    "        chunk = window.strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        i = end\n",
    "\n",
    "        while i < n and text[i] == \" \":\n",
    "            i += 1\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text with CHUNK_SIZE = 60\n",
      "Total chunks created: 4899\n"
     ]
    }
   ],
   "source": [
    "print(f\"Chunking text with CHUNK_SIZE = {CHUNK_SIZE}\")\n",
    "chunked_data = []\n",
    "for line in raw_texts:\n",
    "    chunked_data.extend(chunk_text(line, CHUNK_SIZE))\n",
    "\n",
    "print(\"Total chunks created:\", len(chunked_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Embedding model loaded.\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# 4. Load n-dim embedding model\n",
    "print(f\"\\nLoading embedding model: {EMBED_MODEL_NAME}\")\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "print(\"Embedding model loaded.\")\n",
    "test_vec = embed_model.encode(\"test\", convert_to_numpy=True)\n",
    "print(\"Embedding dimension:\", test_vec.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding chunks (take time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 154/154 [00:03<00:00, 39.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding completed.\n",
      "Vector DB shape: (4899, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Embed all chunks into vector DB\n",
    "print(\"\\nEmbedding chunks (take time)\")\n",
    "vector_db = embed_model.encode(\n",
    "    chunked_data,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "print(\"Embedding completed.\")\n",
    "print(\"Vector DB shape:\", vector_db.shape)   # (num_chunks, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: What is the capital of Finland?\n",
      "\n",
      "Prompt vector (384-dim):\n",
      "[ 4.45507951e-02  3.97122838e-02 -3.90180461e-02  5.03091998e-02\n",
      "  5.06749563e-03 -4.70998064e-02  3.18860374e-02 -3.83177400e-03\n",
      "  3.20874574e-03  3.08345333e-02 -9.64995846e-03 -8.91747177e-02\n",
      " -1.05075665e-01  2.42689606e-02 -6.87031262e-03 -1.66187901e-02\n",
      " -7.03075808e-03  1.70511901e-02  2.71559265e-02  4.10183556e-02\n",
      " -4.28796466e-03 -4.42730822e-02  1.17016668e-02 -2.24054847e-02\n",
      "  1.36240542e-01  6.99829906e-02  1.43763712e-02  5.11771394e-03\n",
      "  6.42202282e-03  3.30357105e-02 -5.42362221e-02 -7.78892264e-02\n",
      "  1.73910148e-03  1.68413576e-02 -4.17772128e-04  6.69443887e-03\n",
      "  4.59495001e-03 -5.04224077e-02  4.32742313e-02 -2.18512081e-02\n",
      " -5.43169677e-03  9.25389002e-04 -7.62411766e-03 -5.73805952e-03\n",
      "  6.59404248e-02  5.14704734e-02 -4.73667048e-02  2.34478414e-02\n",
      "  3.28630246e-02 -2.68838722e-02  4.50134650e-02 -8.59741941e-02\n",
      " -4.01185527e-02 -1.28678521e-02  3.30344774e-02  3.54821086e-02\n",
      " -1.89352110e-02  1.16831940e-02  4.13133092e-02  2.37162560e-02\n",
      " -4.24014032e-03 -9.34008881e-02 -1.19169382e-02  5.40092774e-02\n",
      "  5.05900681e-02 -6.70280084e-02 -5.66458255e-02  1.21104131e-02\n",
      " -5.00861034e-02 -4.23436761e-02  1.15561523e-01 -8.77604559e-02\n",
      " -1.07429363e-02 -2.46386994e-02 -2.62316689e-02 -6.87583312e-02\n",
      " -5.73089533e-02  8.84277001e-02  8.95589532e-04  9.79544222e-02\n",
      "  7.04274848e-02  4.01444174e-02 -6.28227070e-02  7.36777112e-02\n",
      " -1.35115897e-02  5.93890762e-03  5.25100678e-02 -3.78449857e-02\n",
      " -2.32155006e-02 -9.72703658e-03 -1.13090333e-02  2.55837869e-02\n",
      " -3.61140864e-03 -3.12398672e-02 -7.43578970e-02  1.24493251e-02\n",
      " -1.87871531e-02  3.94146219e-02 -3.26709934e-02  4.20476124e-02\n",
      " -1.82732530e-02  4.04489376e-02  2.76400987e-02 -1.13035068e-02\n",
      " -3.16810198e-02  2.89977714e-02  6.11113384e-04 -3.97296548e-02\n",
      "  1.26821492e-02  2.90978816e-03 -6.80402890e-02 -4.77478206e-02\n",
      " -6.10869750e-02 -1.24987271e-02  8.94826651e-02  7.75933564e-02\n",
      "  4.91503887e-02  1.91369038e-02  4.13184315e-02  8.26686695e-02\n",
      " -2.04761289e-02 -6.85835108e-02 -4.75825137e-03  5.84592670e-02\n",
      " -3.25145992e-03  5.91512062e-02 -5.49712963e-02 -6.87796585e-33\n",
      "  2.61990242e-02 -3.07125971e-02  1.52877383e-02  6.67219013e-02\n",
      "  6.09371578e-03 -3.73000428e-02 -2.22608284e-03 -6.81275427e-02\n",
      "  4.91641909e-02  5.47992112e-03 -2.53736451e-02 -1.05699278e-01\n",
      " -4.66483422e-02 -9.17865410e-02  4.21743952e-02  5.69600351e-02\n",
      "  4.20264974e-02 -1.13182040e-02 -6.03457168e-02  5.51739298e-02\n",
      "  4.15788628e-02 -3.23817618e-02 -4.44683209e-02 -3.17345932e-02\n",
      " -7.36039653e-02 -7.54991127e-03  1.34242384e-03 -2.48444788e-02\n",
      "  8.28036368e-02 -7.26816803e-03  2.39549372e-02 -3.76587152e-04\n",
      " -3.17292102e-02  1.70151368e-02 -1.90574247e-02  1.61175672e-02\n",
      " -3.42489518e-02 -6.77116401e-03  3.72678451e-02  6.68061152e-02\n",
      " -8.48273933e-03 -8.22913200e-02 -8.09636712e-02  6.40205517e-02\n",
      "  3.78256515e-02 -1.84637904e-02 -2.26086173e-02 -8.87456015e-02\n",
      "  9.05006230e-02 -2.38047950e-02 -4.51373830e-02  1.50427148e-02\n",
      " -8.47231671e-02  4.37448360e-03  2.16181201e-04  5.30293621e-02\n",
      "  5.65489791e-02  1.44931590e-02  5.57758622e-02  7.67428651e-02\n",
      " -3.83142978e-02 -2.70124413e-02  9.26393550e-03  7.58292945e-03\n",
      "  6.28606528e-02  8.69714469e-02  2.63897926e-02  6.55564107e-03\n",
      "  3.07697374e-02  5.76389022e-03  1.66595113e-02 -6.49737418e-02\n",
      "  2.06080303e-02  4.93138544e-02 -2.98008625e-03 -4.51798458e-03\n",
      "  1.74422737e-03  3.20660993e-02 -3.07834931e-02  2.69403700e-02\n",
      " -5.31211831e-02  4.89190547e-03 -4.12470885e-02  5.38930483e-02\n",
      " -2.56565902e-02 -5.32633997e-02 -5.19664474e-02 -7.23817572e-02\n",
      "  6.42128214e-02 -6.39364123e-02 -7.99264386e-02 -8.93073454e-02\n",
      "  5.53555973e-03 -3.46737243e-02 -1.11431710e-01  4.44383591e-33\n",
      "  9.09088552e-02 -7.05673322e-02 -8.67556036e-02  1.80426519e-02\n",
      " -9.60040018e-02  2.00236607e-02 -2.18718033e-02  4.86329980e-02\n",
      " -8.51115771e-03  8.67507085e-02 -6.41448945e-02 -9.93809253e-02\n",
      "  2.17888393e-02  1.30342871e-01  5.29558025e-02  1.04114108e-01\n",
      "  9.49384645e-02  4.53608856e-02 -3.75485905e-02 -9.15396661e-02\n",
      " -7.42211416e-02  6.66306401e-03 -8.63022730e-03  3.73659804e-02\n",
      " -5.08846343e-03 -2.63719521e-02 -8.46146122e-02 -5.21089956e-02\n",
      " -3.21033038e-02  8.91791843e-03 -6.55752122e-02 -2.16462985e-02\n",
      "  2.59884205e-02  7.71717429e-02 -1.38252014e-02  4.74770255e-02\n",
      "  8.15064982e-02  2.02545356e-02  6.40392350e-03  8.90048072e-02\n",
      "  2.66464595e-02 -5.46320602e-02  6.41683862e-02  1.30045205e-01\n",
      " -2.68273521e-02 -9.04447436e-02  5.15250750e-02  9.93796065e-02\n",
      "  8.55520274e-03 -1.00810798e-02  6.04913048e-02  5.69721498e-02\n",
      " -8.07125941e-02  2.25252733e-02 -2.03339122e-02  4.10871767e-02\n",
      " -3.43874469e-02  3.77644747e-02 -3.86605375e-02 -1.33919083e-02\n",
      "  4.87793386e-02 -7.56008178e-02 -3.63915674e-02  4.10923287e-02\n",
      "  1.58535764e-02  3.21004316e-02 -6.93765581e-02  2.76388023e-02\n",
      " -1.01697911e-02 -2.11365968e-02  4.57743146e-02 -3.76830436e-02\n",
      "  4.00469452e-03  3.55975479e-02 -1.91090368e-02 -2.06770431e-02\n",
      "  1.50543526e-01  5.17591275e-03  7.64458328e-02 -8.35428908e-02\n",
      " -6.77332506e-02 -1.47578577e-02 -3.37593965e-02 -7.87011522e-05\n",
      " -1.15753822e-02  3.56834717e-02  7.40020275e-02 -6.54086024e-02\n",
      " -5.15696630e-02  2.20443332e-03 -1.26731014e-02  8.02913234e-02\n",
      "  5.38950693e-03  1.67175960e-02 -6.48946464e-02 -1.53463926e-08\n",
      "  2.69664656e-02  4.00512554e-02 -3.41546349e-02  1.16040803e-01\n",
      "  1.65737905e-02 -5.23272455e-02  7.29854703e-02 -7.69007346e-03\n",
      " -3.99910584e-02  3.96921635e-02 -7.10060149e-02  7.53021687e-02\n",
      " -1.03933074e-01 -8.49803090e-02  2.45444942e-02  3.52319367e-02\n",
      " -2.69052777e-02  1.34246692e-01  1.32832807e-02  5.06988093e-02\n",
      "  2.56302264e-02  3.43422871e-03 -3.33458968e-02 -7.77368322e-02\n",
      " -6.17946573e-02 -1.81623902e-02  1.17227696e-01  1.28444016e-01\n",
      "  5.24440147e-02 -4.61640209e-02 -1.21709667e-02  1.03264945e-02\n",
      " -6.94205463e-02 -2.22401358e-02 -3.21650021e-02 -5.60561800e-03\n",
      "  1.49347410e-02 -2.16813684e-02 -5.93861938e-02 -9.09030251e-03\n",
      " -4.70266864e-02  4.83154617e-02  3.25570256e-02 -1.88694261e-02\n",
      " -3.06560025e-02  6.53132349e-02  2.83396021e-02 -3.45503166e-02\n",
      "  5.37126623e-02 -8.42407867e-02 -1.18971229e-01  3.64030115e-02\n",
      "  6.67590648e-02  5.33460006e-02 -5.86486384e-02  4.52585658e-03\n",
      "  5.69685828e-03 -4.32986468e-02 -3.14494483e-02  5.68881407e-02\n",
      "  5.94979012e-03 -7.69589189e-03  5.59532009e-02  8.56616627e-03]\n",
      "Vector shape: (384,)\n"
     ]
    }
   ],
   "source": [
    "# 6. Embed the prompt\n",
    "print(\"\\nPrompt:\", PROMPT)\n",
    "prompt_vec = embed_model.encode(PROMPT, convert_to_numpy=True)\n",
    "\n",
    "print(\"\\nPrompt vector (384-dim):\")\n",
    "print(prompt_vec)\n",
    "print(\"Vector shape:\", prompt_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating Euclidean distances\n",
      "CLOSEST MATCH (EUCLIDEAN)\n",
      "__________________________\n",
      "Index: 3687\n",
      "Distance: 1.1241536140441895\n",
      "Matching text chunk:\n",
      "\"Belgium's.\"\n"
     ]
    }
   ],
   "source": [
    "# 7. Compute Euclidean distances\n",
    "print(\"\\nCalculating Euclidean distances\")\n",
    "distances = np.linalg.norm(vector_db - prompt_vec, axis=1)\n",
    "\n",
    "closest_idx = int(np.argmin(distances))\n",
    "closest_chunk = chunked_data[closest_idx]\n",
    "\n",
    "print(\"CLOSEST MATCH (EUCLIDEAN)\")\n",
    "print(\"__________________________\")\n",
    "print(\"Index:\", closest_idx)\n",
    "print(\"Distance:\", float(distances[closest_idx]))\n",
    "print(\"Matching text chunk:\")\n",
    "print(repr(closest_chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 closest chunks to the prompt (Euclidean distance):\n",
      "\n",
      "------------Euclidean distance → lower = better-----------------\n",
      "Rank: 1\n",
      "Index: 3687\n",
      "Distance: 1.124154\n",
      "Chunk:\n",
      "\"Belgium's.\"\n",
      "\n",
      "------------Euclidean distance → lower = better-----------------\n",
      "Rank: 2\n",
      "Index: 2953\n",
      "Distance: 1.133087\n",
      "Chunk:\n",
      "'the Netherlands, New Zealand, Niger, Norway, Oman,'\n",
      "\n",
      "------------Euclidean distance → lower = better-----------------\n",
      "Rank: 3\n",
      "Index: 826\n",
      "Distance: 1.147256\n",
      "Chunk:\n",
      "'in Scandinavian countries.'\n",
      "\n",
      "------------Euclidean distance → lower = better-----------------\n",
      "Rank: 4\n",
      "Index: 4726\n",
      "Distance: 1.149410\n",
      "Chunk:\n",
      "'\"northerner\", and referred to people from Norway, and'\n",
      "\n",
      "------------Euclidean distance → lower = better-----------------\n",
      "Rank: 5\n",
      "Index: 947\n",
      "Distance: 1.151121\n",
      "Chunk:\n",
      "'province of Carinthia.'\n",
      "\n",
      "------------Euclidean distance → lower = better-----------------\n",
      "Rank: 6\n",
      "Index: 2951\n",
      "Distance: 1.157584\n",
      "Chunk:\n",
      "'Belgium, Canada, Czechoslovakia, Denmark, Egypt, France,'\n",
      "\n",
      "------------Euclidean distance → lower = better-----------------\n",
      "Rank: 7\n",
      "Index: 783\n",
      "Distance: 1.178353\n",
      "Chunk:\n",
      "'commonwealth, territory, county, city, municipality, town,'\n",
      "\n",
      "------------Euclidean distance → lower = better-----------------\n",
      "Rank: 8\n",
      "Index: 1024\n",
      "Distance: 1.185898\n",
      "Chunk:\n",
      "'home country, including EU/ EEA countries or any state'\n",
      "\n",
      "------------Euclidean distance → lower = better-----------------\n",
      "Rank: 9\n",
      "Index: 4727\n",
      "Distance: 1.190962\n",
      "Chunk:\n",
      "'northern England and Scotland.'\n",
      "\n",
      "------------Euclidean distance → lower = better-----------------\n",
      "Rank: 10\n",
      "Index: 3615\n",
      "Distance: 1.192698\n",
      "Chunk:\n",
      "'Mongolia.'\n"
     ]
    }
   ],
   "source": [
    "# 8. Show top-K closest matches\n",
    "sorted_indices = np.argsort(distances)\n",
    "\n",
    "print(f\"\\nTop {TOP_K} closest chunks to the prompt (Euclidean distance):\")\n",
    "for rank in range(TOP_K):\n",
    "    idx = int(sorted_indices[rank])\n",
    "    dist = float(distances[idx])\n",
    "    text = chunked_data[idx]\n",
    "\n",
    "    print(\"\\n------------Euclidean distance → lower = better-----------------\")\n",
    "    print(f\"Rank: {rank + 1}\")\n",
    "    print(f\"Index: {idx}\")\n",
    "    print(f\"Distance: {dist:.6f}\")\n",
    "    print(\"Chunk:\")\n",
    "    print(repr(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating Cosine similarities...\n",
      "CLOSEST MATCH (COSINE)\n",
      "__________________________\n",
      "Index: 3687\n",
      "Cosine Score: 0.3681393265724182\n",
      "Matching text chunk:\n",
      "\"Belgium's.\"\n"
     ]
    }
   ],
   "source": [
    "# 7B. Cosine Similarity Search\n",
    "\n",
    "print(\"\\nCalculating Cosine similarities...\")\n",
    "\n",
    "cosine_scores = (vector_db @ prompt_vec) / (\n",
    "    np.linalg.norm(vector_db, axis=1) * np.linalg.norm(prompt_vec)\n",
    ")\n",
    "\n",
    "closest_idx_cosine = int(np.argmax(cosine_scores))\n",
    "closest_chunk_cosine = chunked_data[closest_idx_cosine]\n",
    "\n",
    "print(\"CLOSEST MATCH (COSINE)\")\n",
    "print(\"__________________________\")\n",
    "print(\"Index:\", closest_idx_cosine)\n",
    "print(\"Cosine Score:\", float(cosine_scores[closest_idx_cosine]))\n",
    "print(\"Matching text chunk:\")\n",
    "print(repr(closest_chunk_cosine))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 closest chunks to the prompt (Cosine similarity):\n",
      "\n",
      "---------Cosine similarity → higher = better--------------------\n",
      "Rank: 1\n",
      "Index: 3687\n",
      "Cosine Score: 0.368139\n",
      "Chunk:\n",
      "\"Belgium's.\"\n",
      "\n",
      "---------Cosine similarity → higher = better--------------------\n",
      "Rank: 2\n",
      "Index: 2953\n",
      "Cosine Score: 0.358057\n",
      "Chunk:\n",
      "'the Netherlands, New Zealand, Niger, Norway, Oman,'\n",
      "\n",
      "---------Cosine similarity → higher = better--------------------\n",
      "Rank: 3\n",
      "Index: 826\n",
      "Cosine Score: 0.341902\n",
      "Chunk:\n",
      "'in Scandinavian countries.'\n",
      "\n",
      "---------Cosine similarity → higher = better--------------------\n",
      "Rank: 4\n",
      "Index: 4726\n",
      "Cosine Score: 0.339428\n",
      "Chunk:\n",
      "'\"northerner\", and referred to people from Norway, and'\n",
      "\n",
      "---------Cosine similarity → higher = better--------------------\n",
      "Rank: 5\n",
      "Index: 947\n",
      "Cosine Score: 0.337460\n",
      "Chunk:\n",
      "'province of Carinthia.'\n",
      "\n",
      "---------Cosine similarity → higher = better--------------------\n",
      "Rank: 6\n",
      "Index: 2951\n",
      "Cosine Score: 0.329999\n",
      "Chunk:\n",
      "'Belgium, Canada, Czechoslovakia, Denmark, Egypt, France,'\n",
      "\n",
      "---------Cosine similarity → higher = better--------------------\n",
      "Rank: 7\n",
      "Index: 783\n",
      "Cosine Score: 0.305742\n",
      "Chunk:\n",
      "'commonwealth, territory, county, city, municipality, town,'\n",
      "\n",
      "---------Cosine similarity → higher = better--------------------\n",
      "Rank: 8\n",
      "Index: 1024\n",
      "Cosine Score: 0.296823\n",
      "Chunk:\n",
      "'home country, including EU/ EEA countries or any state'\n",
      "\n",
      "---------Cosine similarity → higher = better--------------------\n",
      "Rank: 9\n",
      "Index: 4727\n",
      "Cosine Score: 0.290804\n",
      "Chunk:\n",
      "'northern England and Scotland.'\n",
      "\n",
      "---------Cosine similarity → higher = better--------------------\n",
      "Rank: 10\n",
      "Index: 3615\n",
      "Cosine Score: 0.288735\n",
      "Chunk:\n",
      "'Mongolia.'\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTop {TOP_K} closest chunks to the prompt (Cosine similarity):\")\n",
    "sorted_cosine_indices = np.argsort(-cosine_scores)  # negative for descending order\n",
    "\n",
    "for rank in range(TOP_K):\n",
    "    idx = int(sorted_cosine_indices[rank])\n",
    "    score = float(cosine_scores[idx])\n",
    "    text = chunked_data[idx]\n",
    "\n",
    "    print(\"\\n---------Cosine similarity → higher = better--------------------\")\n",
    "    print(f\"Rank: {rank + 1}\")\n",
    "    print(f\"Index: {idx}\")\n",
    "    print(f\"Cosine Score: {score:.6f}\")\n",
    "    print(\"Chunk:\")\n",
    "    print(repr(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: semantic_gooaq_minilm/model\n",
      "Embeddings saved to: semantic_gooaq_minilm/embeddings.npy\n",
      "Chunks saved to: semantic_gooaq_minilm/chunks.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np   # you already imported, but just in case\n",
    "\n",
    "OUTPUT_DIR = \"semantic_gooaq_minilm\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Save model locally (so Streamlit can load without downloading)\n",
    "model_path = os.path.join(OUTPUT_DIR, \"model\")\n",
    "embed_model.save(model_path)\n",
    "print(\"Model saved to:\", model_path)\n",
    "\n",
    "# 2) Save embeddings\n",
    "emb_path = os.path.join(OUTPUT_DIR, \"embeddings.npy\")\n",
    "np.save(emb_path, vector_db)\n",
    "print(\"Embeddings saved to:\", emb_path)\n",
    "\n",
    "# 3) Save text chunks\n",
    "chunks_path = os.path.join(OUTPUT_DIR, \"chunks.json\")\n",
    "with open(chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunked_data, f, ensure_ascii=False)\n",
    "print(\"Chunks saved to:\", chunks_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectordb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
